{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc524e1",
   "metadata": {},
   "source": [
    "# NLP Practical Exam — Text Processing + Language Modeling (90 minutes)\n",
    "\n",
    "**Instructions**\n",
    "- Work in this notebook only.\n",
    "- Write short, clear comments to justify *tool choices* (regex vs NLTK, etc.).\n",
    "- Do **not** use external NLP libraries beyond **NLTK**, **NumPy**, **PyTorch** (PyTorch not needed here).\n",
    "- Keep outputs readable (print key variables).\n",
    "\n",
    "**Total: 10 points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16ffbb",
   "metadata": {},
   "source": [
    "## Given text\n",
    "\n",
    "```python\n",
    "text = (\"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.\")\n",
    "```\n",
    "\n",
    "> Treat the text as *synthetic exam data* (no fact-checking needed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f16d8e",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. **(1 pt)** Sentence splitting using **regex + NLTK**.\n",
    "2. **(1 pt)** Regex normalization: acronyms, height meters→centimeters, money `$X.Y billion` → `x point y billion` (words).\n",
    "3. **(1 pt)** Lowercase **except** proper nouns; join multiword proper nouns with underscore (e.g., `Sam Altman → Sam_Altman`). Keep acronyms uppercase.\n",
    "4. **(1 pt)** Tokenize (tool of your choice).\n",
    "5. **(1 pt)** Remove stopwords (tool of your choice); keep entity tokens.\n",
    "6. **(1 pt)** Create bigrams with pure Python.\n",
    "7. **(2 pt)** Build a bigram LM (MLE) and `predict_next(prev_word, top_k=3)`.\n",
    "\n",
    "8. **(2 pt)** Implement a simple **BPE** on: `corpus = \"low lower newest widest\"` (≥5 merges or until no merges).\n",
    "9. **(1 pt)** Compute Accuracy/Precision/Recall/F1 for an invented confusion matrix (explain with comments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c4f7ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLTK downloads (safe to run multiple times)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = (\"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. \"\n",
    "        \"He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. \"\n",
    "        \"A report valued the project at $3.2 billion.\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c9bdf",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2182c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona.', 'He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.']\n"
     ]
    }
   ],
   "source": [
    "# Q1 (1 pt): Sentence splitting (regex + NLTK)\n",
    "# - Use regex to protect acronyms like U.P.C. so they don't break sentence boundaries.\n",
    "# - Then use nltk.sent_tokenize.\n",
    "#\n",
    "# Return: sentences (list of strings)\n",
    "\n",
    "# TODO: implement protect_acronym_dots and restore_acronym_dots (or equivalent)\n",
    "# TODO: apply sent_tokenize\n",
    "\n",
    "# First, I will implement the function to protect acronyms\n",
    "def protect_acronym_dots(text):\n",
    "    # This function replaces the dots in acronyms with a placeholder.\n",
    "    # We need to use lambda to ensure that we only replace the dots in  full acronyms and not in other contexts.\n",
    "    text = re.sub(r\"\\b([A-Z]\\.)+\", lambda m: m.group(0).replace(\".\", \"<DOT>\"), text)\n",
    "    # We also need to protect decimal numbers to avoid breaking them into sentences\n",
    "    text = re.sub(r\"\\b(\\d+)\\.(\\d+)\", r\"\\1<DOT>\\2\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def restore_acronym_dots(text):\n",
    "    # This function restores the dots in acronyms by replacing the placeholder back to dots.\n",
    "    return text.replace(\"<DOT>\", \".\")\n",
    "\n",
    "# Now, I will apply the protect_acronym_dots function to the text\n",
    "protected_text = protect_acronym_dots(text)\n",
    "\n",
    "# Next, I will use nltk.sent_tokenize to split the protected text into sentences\n",
    "sentences = sent_tokenize(protected_text)\n",
    "\n",
    "# Restore the acronym dots in each sentence\n",
    "sentences = [restore_acronym_dots(sentence) for sentence in sentences]\n",
    "\n",
    "# Finally, I will print the final sentences\n",
    "    \n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0cb6c",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ec76b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 186 centimeters tall and met with researchers from UPC and UNESCO A report valued the project at three point two billion.\n"
     ]
    }
   ],
   "source": [
    "# Q2 (1 pt): Regex normalization\n",
    "# Convert:\n",
    "#  - U.P.C. -> UPC, U.N.E.S.C.O. -> UNESCO (general rule: remove dots in acronyms)\n",
    "#  - 1.86m -> 186 centimeters (general: X.YZm -> int(round(float(X.YZ)*100)) centimeters)\n",
    "#  - $3.2 billion -> three point two billion  (digits 0-9 are enough)\n",
    "#\n",
    "# Return: text_norm\n",
    "def normalize_text(text):\n",
    "    # We remove dots WITHIN acronyms but keep the final period if it's sentence-ending\n",
    "    text = re.sub(r\"([A-Z])\\.(?=[A-Z]\\.)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"([A-Z])\\.(?=\\s)\", r\"\\1\", text)\n",
    "    \n",
    "    # We convert meters to centimeters\n",
    "    # I create a function to convert the matched meters to centimeters, and then use re.sub to apply it to all matches in the text\n",
    "    def meters_to_cm(match):\n",
    "        meters = float(match.group(1))\n",
    "        cm = int(round(meters * 100))\n",
    "        return f\"{cm} centimeters\"\n",
    "    text = re.sub(r\"(\\d+\\.\\d+)m\\b\", meters_to_cm, text)\n",
    "       \n",
    "    # We convert digits to words for amounts in billions\n",
    "    def dollar_to_words(match):\n",
    "        number = match.group(1)\n",
    "        unit = match.group(2)\n",
    "        # We use a dictionary to map digits to words, and then we join the words with spaces. \n",
    "        digit_map = {\"0\": \"zero\", \"1\": \"one\", \"2\": \"two\", \"3\": \"three\", \"4\": \"four\",\n",
    "                     \"5\": \"five\", \"6\": \"six\", \"7\": \"seven\", \"8\": \"eight\", \"9\": \"nine\"}\n",
    "        # We also need to handle the decimal point, which we will convert to \"point\".\n",
    "        words = \" \".join(\"point\" if c == \".\" else digit_map.get(c, c) for c in number)\n",
    "        return f\"{words} {unit}\"\n",
    "    text = re.sub(r\"\\$(\\d+\\.?\\d*)\\s+(billion|million|thousand)\", dollar_to_words, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "text_norm = normalize_text(text)\n",
    "print(text_norm)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbaf89",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f2c5a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in mid-February 2026 , the CEO of OpenAI , Sam_Altman , visited barcelona . he is 186 centimeters tall and met with researchers from UPC and UNESCO a report valued the project at three point two billion .\n"
     ]
    }
   ],
   "source": [
    "# Q3 (1 pt): Lowercase except proper nouns + underscore multiword proper nouns\n",
    "# Requirements:\n",
    "# - Convert to lowercase except:\n",
    "#   - Acronyms (ALL CAPS) stay uppercase (e.g., UNESCO, UPC, CEO)\n",
    "#   - MixedCase tokens stay as-is (e.g., OpenAI)\n",
    "#   - Multiword proper nouns joined with underscore (Sam Altman -> Sam_Altman) and preserved\n",
    "#\n",
    "# Return: text_case\n",
    "\n",
    "def lowercase_except_proper_nouns(text):\n",
    "    # We split the text into tokens using word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # We will use a list to store the processed tokens\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        #  We check if the token is an acronym\n",
    "        if token.isupper() and len(token) > 1 and token.isalpha():\n",
    "            processed_tokens.append(token)\n",
    "        #  We check if the token is MixedCase \n",
    "        elif any(c.isupper() for c in token[1:]) and any(c.islower() for c in token):\n",
    "            processed_tokens.append(token)\n",
    "        else:\n",
    "            processed_tokens.append(token.lower())\n",
    "    \n",
    "    # Now we need to join multiword proper nouns with underscores\n",
    "    final_tokens = []\n",
    "    i = 0\n",
    "    while i < len(processed_tokens):\n",
    "        # We check if the current token and the next token are both proper nouns\n",
    "        if (i < len(processed_tokens) - 1 and \n",
    "            tokens[i] and tokens[i][0].isupper() and tokens[i][1:].islower() and tokens[i].isalpha() and\n",
    "            tokens[i+1] and tokens[i+1][0].isupper() and tokens[i+1][1:].islower() and tokens[i+1].isalpha()):\n",
    "            final_tokens.append(tokens[i] + \"_\" + tokens[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            final_tokens.append(processed_tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    return \" \".join(final_tokens)\n",
    "\n",
    "text_case = lowercase_except_proper_nouns(text_norm)\n",
    "\n",
    "print(text_case)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eaf848",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ecd9c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'mid-February', '2026', ',', 'the', 'CEO', 'of', 'OpenAI', ',', 'Sam', 'Altman', ',', 'visited', 'Barcelona', '.', 'He', 'is', '186', 'centimeters', 'tall', 'and', 'met', 'with', 'researchers', 'from', 'UPC', 'and', 'UNESCO', 'A', 'report', 'valued', 'the', 'project', 'at', 'three', 'point', 'two', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "# Q4 (1 pt): Tokenization\n",
    "# Use a tokenizer of your choice (e.g., nltk.word_tokenize).\n",
    "# Return: tokens (list)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # We can use nltk's word_tokenize to tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "tokens = tokenize_text(text_norm)   \n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4e851",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b208cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mid-February', '2026', ',', 'CEO', 'OpenAI', ',', 'Sam', 'Altman', ',', 'visited', 'Barcelona', '.', '186', 'centimeters', 'tall', 'met', 'researchers', 'UPC', 'UNESCO', 'report', 'valued', 'project', 'three', 'point', 'two', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "# Q5 (1 pt): Stopword removal\n",
    "# - Remove English stopwords\n",
    "# - Do NOT remove entity tokens like OpenAI, Sam_Altman, Barcelona, UNESCO, UPC\n",
    "# Return: tokens_nostop\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # We use the function set to create a set of stopwords \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # I create a new list of tokens that only includes those that are not in the stop_words set.\n",
    "    # We need to keep entity tokens like OpenAI, Sam_Altman, Barcelona, UNESCO, UPC.\n",
    "    # In this case, I create a set of entity tokens beacuse they are a small number of this entities.\n",
    "    entity_tokens = {\"OpenAI\", \"Sam_Altman\", \"Barcelona\", \"UNESCO\", \"UPC\"}\n",
    "    tokens_nostop = [token for token in tokens if token.lower() not in stop_words or token in entity_tokens]\n",
    "    return tokens_nostop\n",
    "tokens_nostop = remove_stopwords(tokens)\n",
    "\n",
    "print(tokens_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab209a7",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "94862b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'mid-February'), ('mid-February', '2026'), ('2026', ','), (',', 'the'), ('the', 'CEO'), ('CEO', 'of'), ('of', 'OpenAI'), ('OpenAI', ','), (',', 'Sam'), ('Sam', 'Altman'), ('Altman', ','), (',', 'visited'), ('visited', 'Barcelona'), ('Barcelona', '.'), ('.', 'He'), ('He', 'is'), ('is', '186'), ('186', 'centimeters'), ('centimeters', 'tall'), ('tall', 'and'), ('and', 'met'), ('met', 'with'), ('with', 'researchers'), ('researchers', 'from'), ('from', 'UPC'), ('UPC', 'and'), ('and', 'UNESCO'), ('UNESCO', 'A'), ('A', 'report'), ('report', 'valued'), ('valued', 'the'), ('the', 'project'), ('project', 'at'), ('at', 'three'), ('three', 'point'), ('point', 'two'), ('two', 'billion'), ('billion', '.')]\n",
      "[('mid-February', '2026'), ('2026', ','), (',', 'CEO'), ('CEO', 'OpenAI'), ('OpenAI', ','), (',', 'Sam'), ('Sam', 'Altman'), ('Altman', ','), (',', 'visited'), ('visited', 'Barcelona'), ('Barcelona', '.'), ('.', '186'), ('186', 'centimeters'), ('centimeters', 'tall'), ('tall', 'met'), ('met', 'researchers'), ('researchers', 'UPC'), ('UPC', 'UNESCO'), ('UNESCO', 'report'), ('report', 'valued'), ('valued', 'project'), ('project', 'three'), ('three', 'point'), ('point', 'two'), ('two', 'billion'), ('billion', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Q6 (1 pt): Bigrams with pure Python (no NLTK bigrams helper)\n",
    "# Return: bigrams = [(w1, w2), ...]\n",
    "\n",
    "def create_bigrams(tokens):\n",
    "    # The fisrt thing we do is I create an empty list to store the bigrams\n",
    "    bigrams = []\n",
    "    # Then, I iterate through the tokens and create bigrams by pairing each token with the next one. \n",
    "    for i in range(len(tokens) - 1):\n",
    "        # Here we append a tuple of the current token and the next token to the bigrams list.\n",
    "        bigrams.append((tokens[i], tokens[i+1]))\n",
    "    return bigrams\n",
    "# I didn´t know if you wanted it with or without stopwords, so I created bigrams for both cases.\n",
    "bigrams = create_bigrams(tokens)\n",
    "bigrams_nostop = create_bigrams(tokens_nostop)\n",
    "\n",
    "print(bigrams)\n",
    "print(bigrams_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885728fc",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "51fab00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Q7 (2 pt): Bigram Language Model + next-word prediction\n",
    "# Build:\n",
    "# - bigram_counts[(w1,w2)]\n",
    "# - context_counts[w1]\n",
    "# - model[w1][w2] = P(w2|w1) = count(w1,w2)/count(w1)\n",
    "#\n",
    "# Then implement:\n",
    "# def predict_next(prev_word, model, top_k=3): -> list[(next_word, prob)] sorted\n",
    "\n",
    "def build_bigram_model(bigrams):\n",
    "    # We create dictionaries to store bigram counts and context counts\n",
    "    bigram_counts = defaultdict(int)\n",
    "    context_counts = defaultdict(int)\n",
    "    \n",
    "    # We count the bigrams and contexts\n",
    "    for w1, w2 in bigrams:\n",
    "        bigram_counts[(w1, w2)] += 1\n",
    "        context_counts[w1] += 1\n",
    "    \n",
    "    # We build the probability model P(w2|w1) = count(w1,w2) / count(w1)\n",
    "    model = defaultdict(dict)\n",
    "    for (w1, w2), count in bigram_counts.items():\n",
    "        model[w1][w2] = count / context_counts[w1]\n",
    "    \n",
    "    return model, bigram_counts, context_counts\n",
    "\n",
    "\n",
    "def predict_next(prev_word, model, top_k=3):\n",
    "    # We get all possible next words and their probabilities for the given previous word\n",
    "    if prev_word not in model:\n",
    "        return []\n",
    "    \n",
    "    # We get the dictionary of next words and probabilities\n",
    "    next_words = model[prev_word]\n",
    "    \n",
    "    # We sort by probability in descending order and get top k\n",
    "    sorted_predictions = sorted(next_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # We return the top k predictions as a list of (word, probability) tuples\n",
    "    return sorted_predictions[:top_k]\n",
    "\n",
    "\n",
    "# We build the model using the bigrams we already created\n",
    "model, bigram_counts, context_counts = build_bigram_model(bigrams)\n",
    "\n",
    "\n",
    "\n",
    "# Example:\n",
    "print(predict_next(\"OpenAI\", model, top_k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc35060",
   "metadata": {},
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('l', 'o'), ('lo', 'w'), ('e', 's'), ('es', 't'), ('est', '</w>')]\n"
     ]
    }
   ],
   "source": [
    "# Q8 (2 pt): Simple BPE (Byte Pair Encoding) on a tiny corpus\n",
    "corpus = \"low lower newest widest\"\n",
    "\n",
    "# Requirements:\n",
    "# - Represent each word as characters + </w>\n",
    "# - Compute pair frequencies (weighted by word frequency)\n",
    "# - Merge most frequent pair\n",
    "# - Do at least 5 merges (or stop if no pairs)\n",
    "#\n",
    "# Deliver:\n",
    "# - merges: list of merges in order\n",
    "# - final segmented version of each word\n",
    "\n",
    "# TODO: implement BPE helper functions:\n",
    "# - get_vocab_from_corpus\n",
    "# - get_pair_frequencies\n",
    "# - merge_pair_in_vocab\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "corpus = \"low lower newest widest\"\n",
    "\n",
    "def get_vocab_from_corpus(corpus):\n",
    "    # We split the corpus into words and count their frequencies\n",
    "    words = corpus.split()\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # We represent each word as a list of characters with </w> at the end\n",
    "    vocab = {}\n",
    "    for word, freq in word_freq.items():\n",
    "        vocab[tuple(list(word) + [\"</w>\"])] = freq\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def get_pair_frequencies(vocab):\n",
    "    # We count all adjacent pairs weighted by word frequency\n",
    "    pairs = defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        # We iterate through the characters in each word\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i + 1])\n",
    "            pairs[pair] += freq\n",
    "    \n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d34fa",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "47c22e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 (1 pt): Metrics — Accuracy, Precision, Recall, F1\n",
    "# Invent a confusion matrix (TP, FP, FN, TN) and compute metrics.\n",
    "# Explain each formula briefly in comments.\n",
    "\n",
    "TP = None\n",
    "FP = None\n",
    "FN = None\n",
    "TN = None\n",
    "\n",
    "accuracy = None\n",
    "precision = None\n",
    "recall = None\n",
    "f1 = None\n",
    "\n",
    "# print(accuracy, precision, recall, f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
