{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Words, Tokens, and Regular Expressions\n",
    "## Exercises Notebook - Session 2\n",
    "\n",
    "This notebook contains exercises covering:\n",
    "- Word and token counting\n",
    "- Regular expressions in Python\n",
    "- Unicode handling\n",
    "- Unix text processing tools\n",
    "- Tokenization concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Word and Token Counting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Counting Words\n",
    "\n",
    "Given the sentence from the slides: *\"They picnicked by the pool, then lay back on the grass and looked at the stars.\"*\n",
    "\n",
    "Write Python code to:\n",
    "1. Count words excluding punctuation\n",
    "2. Count words including punctuation as separate tokens\n",
    "3. Count unique word types (case-insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words excluding punctuation: ['They', 'picnicked', 'by', 'the', 'pool', 'then', 'lay', 'back', 'on', 'the', 'grass', 'and', 'looked', 'at', 'the', 'stars']\n",
      "Number of words excluding punctuation: 16\n",
      "Words including punctuation: ['They', 'picnicked', 'by', 'the', 'pool,', 'then', 'lay', 'back', 'on', 'the', 'grass', 'and', 'looked', 'at', 'the', 'stars.']\n",
      "Number of words including punctuation: 16\n",
      "Unique types (case-insensitive): {'they', 'lay', 'grass', 'the', 'picnicked', 'at', 'back', 'by', 'on', 'pool', 'and', 'stars', 'looked', 'then'}\n",
      "Number of unique types (case-insensitive): 14\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "sentence = \"They picnicked by the pool, then lay back on the grass and looked at the stars.\"\n",
    "import re\n",
    "\n",
    "# 1. Count words excluding punctuation\n",
    "words_no_punct = re.findall(r\"\\b\\w+\\b\", sentence)\n",
    "print(f\"Words excluding punctuation: {words_no_punct}\")\n",
    "# We need to use len() to count the number of words excluding punctuation\n",
    "print(f\"Number of words excluding punctuation: {len(words_no_punct)}\")\n",
    "\n",
    "# 2. Count words including punctuation\n",
    "words_with_punct = re.findall(r\"\\S+\", sentence)\n",
    "print(f\"Words including punctuation: {words_with_punct}\")\n",
    "# We need to use len() to count the number of words including punctuation\n",
    "print(f\"Number of words including punctuation: {len(words_with_punct)}\")\n",
    "\n",
    "# 3. Count unique types (case-insensitive)\n",
    "unique_types = set(word.lower() for word in words_no_punct)\n",
    "print(f\"Unique types (case-insensitive): {unique_types}\")\n",
    "# We need to use len() to count the number of unique types\n",
    "print(f\"Number of unique types (case-insensitive): {len(unique_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Handling Disfluencies\n",
    "\n",
    "The slides show the utterance: *\"I do uh main- mainly business data processing\"*\n",
    "\n",
    "Write code to:\n",
    "1. Count all tokens including disfluencies\n",
    "2. Remove filled pauses (uh, um) and fragments (words ending with -)\n",
    "3. Count \"clean\" words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens including disfluencies: ['I', 'do', 'uh', 'main-', 'mainly', 'business', 'data', 'processing']\n",
      "Number of tokens including disfluencies: 8\n",
      "Tokens after removing disfluencies: ['I', 'do', 'mainly', 'business', 'data', 'processing']\n",
      "Number of clean words: 6\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "utterance = \"I do uh main- mainly business data processing\"\n",
    "\n",
    "# 1. Count all tokens including disfluencies\n",
    "# We can use the split() method to tokenize the utterance by whitespace\n",
    "tokens_with_disfluencies = utterance.split()\n",
    "print(f\"Tokens including disfluencies: {tokens_with_disfluencies}\")\n",
    "# We need to use len() to count the number of tokens including disfluencies\n",
    "print(f\"Number of tokens including disfluencies: {len(tokens_with_disfluencies)}\")\n",
    "\n",
    "# 2. Remove filled pauses (uh, um) and fragments (words ending with -)\n",
    "# We can use a list comprehension eliminate filled pauses and fragments\n",
    "tokens_cleaned = [token for token in tokens_with_disfluencies if token not in {\"uh\", \"um\"} and not token.endswith(\"-\")]\n",
    "print(f\"Tokens after removing disfluencies: {tokens_cleaned}\")\n",
    "\n",
    "# 3. Count \"clean\" words\n",
    "# We can use len() to count the number of clean words\n",
    "print(f\"Number of clean words: {len(tokens_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Regular Expressions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Basic Pattern Matching\n",
    "\n",
    "Using the regex patterns from the slides, write code to:\n",
    "1. Find all words starting with capital letters\n",
    "2. Find all digits in a text\n",
    "3. Find words that are NOT capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "text = \"Chapter 1: Down the Rabbit Hole. Alice was 7 years old in 1865.\"\n",
    "\n",
    "# 1. Find all words starting with capital letters\n",
    "capitalized_words = re.findall(r\"\\b[A-Z][a-zA-Z]*\\b\", text)\n",
    "print(f\"Capitalized words: {capitalized_words}\")\n",
    "\n",
    "# 2. Find all digits in a text\n",
    "digits = re.findall(r\"\\d+\", text)\n",
    "print(f\"Digits in the text: {digits}\")\n",
    "\n",
    "# 3. Find words that are NOT capitalized\n",
    "non_capitalized_words = re.findall(r\"\\b[a-z][a-zA-Z]*\\b\", text)\n",
    "print(f\"Non-capitalized words: {non_capitalized_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Kleene Star and Plus\n",
    "\n",
    "The slides explain Kleene operators (* and +).\n",
    "\n",
    "Write patterns to match:\n",
    "1. \"baa\" followed by zero or more 'a's (baa, baaa, baaaa...)\n",
    "2. One or more digits\n",
    "3. Any sequence of characters (wildcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings matching 'baa' followed by zero or more 'a's: ['baa', 'baaa', 'baaaa', 'baaaaa']\n",
      "Strings matching one or more digits: []\n",
      "Strings matching any sequence of characters: ['ba', 'baa', 'baaa', 'baaaa', 'baaaaa']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "test_strings = [\"ba\", \"baa\", \"baaa\", \"baaaa\", \"baaaaa\"]\n",
    "\n",
    "# 1. \"baa\" followed by zero or more 'a's (baa, baaa, baaaa...)\n",
    "pattern_1 = r\"^baa+$\"\n",
    "matches_1 = [s for s in test_strings if re.match(pattern_1, s)]\n",
    "print(f\"Strings matching 'baa' followed by zero or more 'a's: {matches_1}\")\n",
    "\n",
    "# 2. One or more digits\n",
    "pattern_2 = r\"^\\d+$\"\n",
    "matches_2 = [s for s in test_strings if re.match(pattern_2, s)]\n",
    "print(f\"Strings matching one or more digits: {matches_2}\")\n",
    "\n",
    "# 3. Any sequence of characters (wildcard)\n",
    "pattern_3 = r\"^.*$\"\n",
    "matches_3 = [s for s in test_strings if re.match(pattern_3, s)]\n",
    "print(f\"Strings matching any sequence of characters: {matches_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Anchors and Word Boundaries\n",
    "\n",
    "Write patterns using anchors (^, $, \\b) to:\n",
    "1. Match lines starting with a capital letter\n",
    "2. Match lines ending with a period\n",
    "3. Find the word \"the\" as a complete word (not in \"other\" or \"there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "lines = [\n",
    "    \"The quick brown fox\",\n",
    "    \"jumped over the lazy dog.\",\n",
    "    \"other animals watched.\",\n",
    "    \"There was nothing else.\"\n",
    "]\n",
    "\n",
    "# 1. Match lines starting with a capital letter\n",
    "# We use the caret (^) to indicate the start of the line and [A-Z] to match any capital letter\n",
    "pattern_start_capital = r\"^[A-Z].*\"\n",
    "lines_starting_with_capital = [line for line in lines if re.match(pattern_start_capital, line)]\n",
    "print(f\"Lines starting with a capital letter: {lines_starting_with_capital}\")\n",
    "\n",
    "# 2. Match lines ending with a period\n",
    "# We use the dollar sign ($) to indicate the end of the line and \\. to match a literal period\n",
    "pattern_end_period = r\".*\\.$\"\n",
    "lines_ending_with_period = [line for line in lines if re.match(pattern_end_period, line)]\n",
    "print(f\"Lines ending with a period: {lines_ending_with_period}\")\n",
    "\n",
    "# 3. Find the word \"the\" as a complete word (not in \"other\" or \"there\")\n",
    "# We use \\b to indicate word boundaries around \"the\"\n",
    "pattern_whole_word_the = r\"\\bthe\\b\"\n",
    "lines_with_whole_word_the = [line for line in lines if re.search(pattern_whole_word_the, line, re.IGNORECASE)]\n",
    "print(f\"Lines containing the whole word 'the': {lines_with_whole_word_the}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Substitutions and Capture Groups\n",
    "\n",
    "The slides show date format conversion. Write code to:\n",
    "1. Convert US dates (mm/dd/yyyy) to EU format (dd-mm-yyyy)\n",
    "2. Anonymize email addresses by replacing with [EMAIL]\n",
    "3. Convert \"I'm\" contractions to \"I am\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "text_dates = \"Meeting on 10/15/2011 and 03/22/2024\"\n",
    "text_emails = \"Contact john@example.com or jane@test.org\"\n",
    "text_contractions = \"I'm happy and I'm excited\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1. Date format conversion (exact example from slides)\n",
    "text_dates = \"The date is 10/15/2011\"\n",
    "converted = re.sub(r'(\\d{2})/(\\d{2})/(\\d{4})', r'\\2-\\1-\\3', text_dates)\n",
    "print(f\"1. US to EU date: '{text_dates}' -> '{converted}'\")\n",
    "\n",
    "# 2. Email anonymization\n",
    "text_emails = \"Contact john@example.com or jane@test.org\"\n",
    "anon = re.sub(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', '[EMAIL]', text_emails)\n",
    "print(f\"\\n2. Anonymized: '{anon}'\")\n",
    "\n",
    "# 3. Contraction expansion\n",
    "text_contractions = \"I'm happy and I'm excited\"\n",
    "expanded = re.sub(r\"I'm\", \"I am\", text_contractions)\n",
    "print(f\"\\n3. Expanded: '{expanded}'\")\n",
    "\n",
    "# EXPLANATION (Slides reference):\n",
    "# Capture groups use parentheses () to store matched values.\n",
    "# In the replacement string:\n",
    "# - \\1 refers to first captured group (month)\n",
    "# - \\2 refers to second captured group (day)\n",
    "# - \\3 refers to third captured group (year)\n",
    "# The slides show: re.sub(r\"(\\d{2})/(\\d{2})/(\\d{4})\", r\"\\2-\\1-\\3\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5: ELIZA-style Pattern Matching\n",
    "\n",
    "The slides show ELIZA, an early chatbot using regex.\n",
    "Implement simple ELIZA rules for:\n",
    "1. \"I'm [emotion]\" -> \"WHY DO YOU THINK YOU ARE [emotion]\"\n",
    "2. \"I need [something]\" -> \"WHAT WOULD IT MEAN IF YOU GOT [something]\"\n",
    "3. Any input with \"always\" -> \"CAN YOU THINK OF A SPECIFIC EXAMPLE?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the rules for a simple ELIZA-like\n",
    "\n",
    "# 1. \"I'm [emotion]\" -> \"WHY DO YOU THINK YOU ARE [emotion]\"\n",
    "# 2. \"I need [something]\" -> \"WHAT WOULD IT MEAN IF YOU GOT [something]\"\n",
    "# 3. Any input with \"always\" -> \"CAN YOU THINK OF A SPECIFIC EXAMPLE?\n",
    "\n",
    "def simple_eliza(user_input):\n",
    "    # Implement ELIZA rules\n",
    "    # Rule 1\n",
    "    if match := re.match(r\"I'm (.+)\", user_input, re.IGNORECASE):\n",
    "        emotion = match.group(1)\n",
    "        return f\"WHY DO YOU THINK YOU ARE {emotion.upper()}\"\n",
    "    # Rule 2\n",
    "    elif match := re.match(r\"I need (.+)\", user_input, re.IGNORECASE):\n",
    "        something = match.group(1)\n",
    "        return f\"WHAT WOULD IT MEAN IF YOU GOT {something.upper()}\"\n",
    "    # Rule 3\n",
    "    elif re.search(r\"always\", user_input, re.IGNORECASE):\n",
    "        return \"CAN YOU THINK OF A SPECIFIC EXAMPLE?\"\n",
    "    else:\n",
    "        return \"PLEASE TELL ME MORE.\"\n",
    "\n",
    "# Test\n",
    "test_inputs = [\n",
    "    \"I'm depressed\",\n",
    "    \"I need a vacation\", \n",
    "    \"They always ignore me\"\n",
    "]\n",
    "# Now we can test the function with the test inputs\n",
    "for input_str in test_inputs:\n",
    "    response = simple_eliza(input_str)\n",
    "    print(f\"User: {input_str}\\nELIZA: {response}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Unicode and Encoding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Unicode Code Points\n",
    "\n",
    "The slides explain that Unicode assigns code points to characters.\n",
    "Write code to:\n",
    "1. Get the code point of 'a' (should be U+0061)\n",
    "2. Get the character for code point U+00F1 (Ã±)\n",
    "3. Show the UTF-8 byte encoding of \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code point of 'a': U+0061\n",
      "Character for code point U+00F1: Ã±\n",
      "UTF-8 byte encoding of 'hello': b'hello'\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the code point of 'a' (should be U+0061)\n",
    "# We can use the ord() function to get the Unicode code point of a character\n",
    "code_point_a = ord('a')\n",
    "print(f\"Code point of 'a': U+{code_point_a:04X}\")\n",
    "\n",
    "# 2. Get the character for code point U+00F1 (Ã±)\n",
    "# We can use the chr() function to get the character from a Unicode code point\n",
    "char_Ã± = chr(0x00F1)\n",
    "print(f\"Character for code point U+00F1: {char_Ã±}\")\n",
    "\n",
    "# 3. Show the UTF-8 byte encoding of \"hello\"\n",
    "# We can use the encode() method to get the UTF-8 byte encoding of a string\n",
    "utf8_hello = \"hello\".encode('utf-8')\n",
    "print(f\"UTF-8 byte encoding of 'hello': {utf8_hello}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Multi-byte UTF-8 Characters\n",
    "\n",
    "The slides show that different scripts need different byte lengths.\n",
    "Examine the byte lengths for:\n",
    "1. ASCII character: 'A'\n",
    "2. Spanish: 'Ã±' (U+00F1)\n",
    "3. Chinese: 'å§š' (a character from the slides)\n",
    "4. Emoji: 'ðŸ˜€'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte length of ASCII character 'A': 1 bytes\n",
      "Byte length of Spanish character 'Ã±': 2 bytes\n",
      "Byte length of Chinese character 'å§š': 3 bytes\n",
      "Byte length of emoji character 'ðŸ˜€': 4 bytes\n"
     ]
    }
   ],
   "source": [
    "# Examine the byte lengths for:\n",
    "# 1. ASCII character: 'A'\n",
    "ascii_char = \"A\"\n",
    "# We can use the encode() method to get the UTF-8 byte encoding of the ASCII character\n",
    "ascii_bytes = ascii_char.encode(\"utf-8\")\n",
    "# We need to use len() to get the byte length\n",
    "print(f\"Byte length of ASCII character 'A': {len(ascii_bytes)} bytes\")\n",
    "\n",
    "# 2. Spanish: 'Ã±' (U+00F1)\n",
    "spanish_char = \"Ã±\"\n",
    "# We can use the encode() method to get the UTF-8 byte encoding of the Spanish character\n",
    "spanish_bytes = spanish_char.encode(\"utf-8\")\n",
    "# We need to use len() to get the byte length\n",
    "print(f\"Byte length of Spanish character 'Ã±': {len(spanish_bytes)} bytes\")\n",
    "\n",
    "# 3. Chinese: 'å§š' (a character from the slides)\n",
    "chinese_char = \"å§š\"\n",
    "# We can use the encode() method to get the UTF-8 byte encoding of the Chinese character\n",
    "chinese_bytes = chinese_char.encode(\"utf-8\")\n",
    "# We need to use len() to get the byte length\n",
    "print(f\"Byte length of Chinese character 'å§š': {len(chinese_bytes)} bytes\")\n",
    "\n",
    "# 4. Emoji: 'ðŸ˜€'\n",
    "emoji_char = \"ðŸ˜€\"\n",
    "# We can use the encode() method to get the UTF-8 byte encoding of the emoji character\n",
    "emoji_bytes = emoji_char.encode(\"utf-8\")\n",
    "# We need to use len() to get the byte length\n",
    "print(f\"Byte length of emoji character 'ðŸ˜€': {len(emoji_bytes)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: String Length vs Byte Length\n",
    "\n",
    "The slides note that len() returns code points, not bytes.\n",
    "Compare string length vs byte length for mixed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String length: 10 characters\n",
      "Byte length (UTF-8): 18 bytes\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "mixed_text = \"SeÃ±or ä½ å¥½ ðŸ˜€\"\n",
    "\n",
    "# Compare string length vs byte length for mixed text.\n",
    "# For string length, we can use len() directly on the string\n",
    "string_length = len(mixed_text)\n",
    "# For byte length, we encode the string to UTF-8 and then use len()\n",
    "byte_length = len(mixed_text.encode(\"utf-8\"))\n",
    "print(f\"String length: {string_length} characters\")\n",
    "print(f\"Byte length (UTF-8): {byte_length} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Shell Commands for Text Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Basic tr Command\n",
    "\n",
    "The slides show Unix tools for tokenization.\n",
    "Using shell commands, tokenize text by:\n",
    "1. Converting all characters to lowercase\n",
    "2. Replacing non-alphabetic chars with newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "# Create a sample file first\n",
    "sample_text = \"\"\"THE SONNETS\n",
    "by William Shakespeare\n",
    "From fairest creatures\n",
    "We desire increase\"\"\"\n",
    "\n",
    "with open('sample.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# YOUR SHELL COMMANDS HERE (use ! prefix in Jupyter)\n",
    "# 1. Converting all characters to lowercase\n",
    "# We use the wsl prefix to run the command in WSL environment\n",
    "! wsl cat sample.txt | wsl tr 'A-Z' 'a-z'\n",
    "\n",
    "# 2. Replacing non-alphabetic chars with newlines\n",
    "! wsl sed 's/[^a-zA-Z]/\\n/g' sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Word Frequency Count\n",
    "\n",
    "Implement the complete pipeline from the slides:\n",
    "```\n",
    "tr -sc 'A-Za-z' '\\n' < file | sort | uniq -c | sort -n -r\n",
    "```\n",
    "This should output word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - implement in shell or Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: The Mystery 'd'\n",
    "\n",
    "The slides show that in Shakespeare's text, 'd' appears 8954 times.\n",
    "The slide asks \"What happened here?\"\n",
    "\n",
    "Explain why and write code to investigate contractions like \"'d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
