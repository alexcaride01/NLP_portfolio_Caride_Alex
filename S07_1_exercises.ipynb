{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S07 - Word Embeddings & Neural Networks\n",
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1 (Easy)\n",
        "Load pre-trained Word2Vec embeddings and find similar words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "[('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained word2vec (google-news-300 or glove-wiki-gigaword-100)\n",
        "# Find top 5 most similar words to 'king'\n",
        "\n",
        "# We load the 'glove-wiki-gigaword-100' model\n",
        "model = api.load('glove-wiki-gigaword-100')\n",
        "similar_words = model.most_similar('king', topn=5)\n",
        "print(similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2 (Easy)\n",
        "Perform word analogy: king - man + woman = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "king - man + woman = [('queen', 0.7698540687561035)]\n",
            "paris - france + spain = [('madrid', 0.8061118125915527)]\n"
          ]
        }
      ],
      "source": [
        "# Use the model to solve: king - man + woman = ?\n",
        "# Also try: paris - france + spain = ?\n",
        "\n",
        "result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
        "print(\"king - man + woman =\", result)\n",
        "result2 = model.most_similar(positive=['paris', 'spain'], negative=['france'], topn=1)\n",
        "print(\"paris - france + spain =\", result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3 (Medium)\n",
        "Train your own Word2Vec model on a custom corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = [\n",
        "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"dog\", \"ran\", \"in\", \"the\", \"park\"],\n",
        "    [\"cats\", \"and\", \"dogs\", \"are\", \"pets\"],\n",
        "    [\"the\", \"cat\", \"chased\", \"the\", \"dog\"],\n",
        "    [\"pets\", \"need\", \"food\", \"and\", \"water\"]\n",
        "]\n",
        "\n",
        "# Train Word2Vec model (vector_size=50, window=3, min_count=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4 (Medium)\n",
        "Build a simple neural network for text classification using embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        # Define: embedding layer, linear layer\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Embed -> mean pooling -> classify\n",
        "        pass\n",
        "\n",
        "# Test with dummy data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5 (Hard)\n",
        "Implement the Skip-gram model from scratch (forward pass only).\n",
        "\n",
        "*Research: Skip-gram predicts context words given center word.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # Two embedding matrices: center and context\n",
        "        pass\n",
        "    \n",
        "    def forward(self, center, context):\n",
        "        # Return dot product scores\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "procesamiento",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
