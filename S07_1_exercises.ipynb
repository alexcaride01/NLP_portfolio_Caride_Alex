{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S07 - Word Embeddings & Neural Networks\n",
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1 (Easy)\n",
        "Load pre-trained Word2Vec embeddings and find similar words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "[('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained word2vec (google-news-300 or glove-wiki-gigaword-100)\n",
        "# Find top 5 most similar words to 'king'\n",
        "\n",
        "# We load the 'glove-wiki-gigaword-100' model\n",
        "model = api.load('glove-wiki-gigaword-100')\n",
        "similar_words = model.most_similar('king', topn=5)\n",
        "print(similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2 (Easy)\n",
        "Perform word analogy: king - man + woman = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "king - man + woman = [('queen', 0.7698540687561035)]\n",
            "paris - france + spain = [('madrid', 0.8061118125915527)]\n"
          ]
        }
      ],
      "source": [
        "# Use the model to solve: king - man + woman = ?\n",
        "# Also try: paris - france + spain = ?\n",
        "\n",
        "result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
        "print(\"king - man + woman =\", result)\n",
        "result2 = model.most_similar(positive=['paris', 'spain'], negative=['france'], topn=1)\n",
        "print(\"paris - france + spain =\", result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3 (Medium)\n",
        "Train your own Word2Vec model on a custom corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector for 'cat': [ 0.0001904   0.00615311 -0.01362527 -0.00274993  0.01533661  0.01469298\n",
            " -0.00734422  0.0052875  -0.0166348   0.01241069 -0.00927603 -0.00632881\n",
            "  0.01862341  0.00174694  0.01498011 -0.01214779  0.0103222   0.0198463\n",
            " -0.01691667 -0.01027177 -0.01413098 -0.00972589 -0.00755462 -0.01707052\n",
            "  0.01591139 -0.00969053  0.01684555  0.01052465 -0.013102    0.00791471\n",
            "  0.01093842 -0.01485397 -0.01480907 -0.00495215 -0.01725133 -0.00316367\n",
            " -0.00080567  0.0065959   0.0028836  -0.00176379 -0.01119006  0.00346125\n",
            " -0.00179504  0.01358557  0.00794846  0.00906098  0.00286886 -0.00539834\n",
            " -0.0087339  -0.00206309]\n",
            "Vector for 'dog': [ 1.56351421e-02 -1.90203730e-02 -4.11062239e-04  6.93839323e-03\n",
            " -1.87794445e-03  1.67635437e-02  1.80215668e-02  1.30730132e-02\n",
            " -1.42324204e-03  1.54208085e-02 -1.70686692e-02  6.41421322e-03\n",
            " -9.27599426e-03 -1.01779103e-02  7.17923651e-03  1.07406788e-02\n",
            "  1.55390287e-02 -1.15330126e-02  1.48667218e-02  1.32509926e-02\n",
            " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195115e-02\n",
            " -1.57510047e-03 -1.34197120e-02 -1.41718509e-02 -4.99412045e-03\n",
            "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
            "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975407e-03\n",
            "  4.88066689e-05 -1.97670180e-02  5.38400887e-03 -9.50021297e-03\n",
            "  2.17529293e-03 -3.15244915e-03  4.39334614e-03 -1.57631524e-02\n",
            " -5.43436781e-03  5.32639725e-03  1.06933638e-02 -4.78302967e-03\n",
            " -1.90201886e-02  9.01175756e-03]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = [\n",
        "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"dog\", \"ran\", \"in\", \"the\", \"park\"],\n",
        "    [\"cats\", \"and\", \"dogs\", \"are\", \"pets\"],\n",
        "    [\"the\", \"cat\", \"chased\", \"the\", \"dog\"],\n",
        "    [\"pets\", \"need\", \"food\", \"and\", \"water\"]\n",
        "]\n",
        "\n",
        "# Train Word2Vec model (vector_size=50, window=3, min_count=1)\n",
        "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1)\n",
        "\n",
        "#  We find the vector for 'cat' and 'dog'\n",
        "cat_vector = model.wv['cat']\n",
        "dog_vector = model.wv['dog']\n",
        "\n",
        "print(\"Vector for 'cat':\", cat_vector)\n",
        "print(\"Vector for 'dog':\", dog_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4 (Medium)\n",
        "Build a simple neural network for text classification using embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([1, 2])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        # Define: embedding layer, linear layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Embed -> mean pooling -> classify\n",
        "        embedded = self.embedding(x)\n",
        "        pooled = embedded.mean(dim=1)\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n",
        "\n",
        "# Test with dummy data\n",
        "vocab_size = 1000\n",
        "embed_dim = 128\n",
        "num_classes = 2\n",
        "\n",
        "model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
        "dummy_input = torch.randint(0, vocab_size, (1, 10))  # 1 is for batch size and 10 is sequence length\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5 (Hard)\n",
        "Implement the Skip-gram model from scratch (forward pass only).\n",
        "\n",
        "*Research: Skip-gram predicts context words given center word.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # Two embedding matrices: center and context\n",
        "        pass\n",
        "    \n",
        "    def forward(self, center, context):\n",
        "        # Return dot product scores\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "procesamiento",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
