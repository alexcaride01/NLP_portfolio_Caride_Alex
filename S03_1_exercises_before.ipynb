{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting text into sentences and words when dealing with complex real-world text containing dates, amounts, URLs, emails, acronyms, and multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a text variable, split it into:\n",
    "1. **Sentences** - logical units of meaning ending with terminal punctuation\n",
    "2. **Words (tokens)** - individual meaningful units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function that Given a text variable, split it into:\n",
    "# Sentences- logical units of meaning ending with terminal punctuation\n",
    "# Words (tokens) - individual meaningful units\n",
    "\n",
    "import re\n",
    "def split_text(text):\n",
    "    # First we split the text to find all the words\n",
    "    words = re.findall(r\"[^\\s,]+(?:,[^\\s,]+)*\", text)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n",
      "\n",
      "Words:\n",
      "['Dr.', 'John', 'Smith', 'Ph.D.', 'earned', '$1,250.50', 'on', 'Jan.', '15', '2024', 'for', 'his', 'work', 'at', 'A.I.', 'Corp.', 'You', 'can', 'reach', 'him', 'at', 'j.smith@ai-corp.co.uk', 'or', 'visit', 'https://www.ai-corp.co.uk/team/dr-smith', 'for', 'more', 'info.', 'The', 'U.S.A.-based', 'company', 'reported', 'a', '23.5%', 'increase', 'in', 'Q3', 'revenue', 'totaling', '€2.5M.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text with challenging elements\n",
    "text = \"\"\"Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "# Split the text into words\n",
    "words = split_text(text)\n",
    "print(\"\\nWords:\")\n",
    "print(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unix tools is not useful for this exercise\n",
    "# Unix tools like grep, sed, and awk are not suitable for this tokenization exercise because they lack linguistic intelligence\n",
    "# and work only with simple pattern matching. While we can extract words using regex patterns, these tools cannot reliably split\n",
    "# sentences because they treat all periods identically, incorrectly breaking abbreviations like \"Dr. Smith\" or \"Ph.D.\" into separate segments.\n",
    "#Unlike NLTK's Punkt algorithm or spaCy's neural models, which are trained on real text corpora to distinguish abbreviation periods from \n",
    "# sentence-ending periods, Unix tools have no contextual awareness or learning capability. We therefore exclude Unix tools because proper\n",
    "# sentence segmentation requires specialized linguistic knowledge that only dedicated NLP libraries provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences (NLTK):\n",
      "['Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I.', 'Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.', 'The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.']\n",
      "\n",
      "Words (NLTK):\n",
      "['Dr.', 'John', 'Smith', 'Ph.D.', 'earned', '$1,250.50', 'on', 'Jan.', '15', '2024', 'for', 'his', 'work', 'at', 'A.I.', 'Corp.', 'You', 'can', 'reach', 'him', 'at', 'j.smith@ai-corp.co.uk', 'or', 'visit', 'https://www.ai-corp.co.uk/team/dr-smith', 'for', 'more', 'info.', 'The', 'U.S.A.-based', 'company', 'reported', 'a', '23.5%', 'increase', 'in', 'Q3', 'revenue', 'totaling', '€2.5M.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\carid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Now we try with NLTK \n",
    "import nltk\n",
    "# This is needed to download the punkt tokenizer models\n",
    "nltk.download(\"punkt_tab\")\n",
    "#  We import the necessary functions\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "def nltk_split_text(text):\n",
    "    # We use NLTK's built-in functions to split the text, in this case in sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # We create a custom tokenizer using our specific regex pattern\n",
    "    # This pattern keeps dots in abbreviations and commas inside numbers\n",
    "    word_tokenizer = RegexpTokenizer(r\"[^\\s,]+(?:,[^\\s,]+)*\")\n",
    "    # And in words\n",
    "    words = word_tokenizer.tokenize(text)\n",
    "    return sentences, words\n",
    "\n",
    "# Now we try with NLTK\n",
    "sentences, words = nltk_split_text(text)\n",
    "print(\"\\nSentences (NLTK):\")\n",
    "print(sentences)\n",
    "print(\"\\nWords (NLTK):\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences (SpaCy):\n",
      "['Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp.', 'You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.', 'The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.']\n",
      "\n",
      "Words (SpaCy):\n",
      "['Dr.', 'John', 'Smith', 'Ph.D.', 'earned', '$1,250.50', 'on', 'Jan.', '15', '2024', 'for', 'his', 'work', 'at', 'A.I.', 'Corp.', 'You', 'can', 'reach', 'him', 'at', 'j.smith@ai-corp.co.uk', 'or', 'visit', 'https://www.ai-corp.co.uk/team/dr-smith', 'for', 'more', 'info', '.', 'The', 'U.S.A.-based', 'company', 'reported', 'a', '23.5', '%', 'increase', 'in', 'Q3', 'revenue', 'totaling', '€2.5M.']\n"
     ]
    }
   ],
   "source": [
    "# Now we try with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def spacy_split_text(text):\n",
    "    # We process the text with spaCy's neural pipeline\n",
    "    doc = nlp(text)\n",
    "    # We extract sentences using dependency-based segmentation\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    # We filter out standalone comma tokens\n",
    "    # SpaCy separates everything, so we need to remove commas manually\n",
    "    words = [token.text for token in doc if token.text != ',']\n",
    "    # We have also to take into account that $ needs to be attached to the number\n",
    "    # So we merge $ with the next token if applicable\n",
    "    # We create a new list to hold the filtered words\n",
    "    filtered_words = []\n",
    "    # We use a variable to skip the next token if we have merged it\n",
    "    skip_next = False\n",
    "    for i, token in enumerate(words):\n",
    "        # If we have to skip the next token, we do it\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        # If the token is $ or € , we merge it with the next token\n",
    "        if token == \"$\" or token == \"€\" and i + 1 < len(words):\n",
    "            filtered_words.append(token + words[i + 1])\n",
    "            skip_next = True\n",
    "        # Otherwise, we just add the token\n",
    "        else:\n",
    "            filtered_words.append(token)\n",
    "    words = filtered_words\n",
    "\n",
    "    \n",
    "    return sentences, words\n",
    "\n",
    "# Now we try with SpaCy\n",
    "sentences, words = spacy_split_text(text)\n",
    "print(\"\\nSentences (SpaCy):\")\n",
    "print(sentences)\n",
    "print(\"\\nWords (SpaCy):\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting words in large corpuses and find the most common words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a file `shakes.txt` in the book folder. Find the words that are more common in Shakespeare's book. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
