{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting text into sentences and words when dealing with complex real-world text containing dates, amounts, URLs, emails, acronyms, and multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a text variable, split it into:\n",
    "1. **Sentences** - logical units of meaning ending with terminal punctuation\n",
    "2. **Words (tokens)** - individual meaningful units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function that Given a text variable, split it into:\n",
    "# Sentences- logical units of meaning ending with terminal punctuation\n",
    "# Words (tokens) - individual meaningful units\n",
    "\n",
    "import re\n",
    "def split_text(text):\n",
    "    # First we split the text to find all the words\n",
    "    words = re.findall(r\"[^\\s,]+(?:,[^\\s,]+)*\", text)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n",
      "\n",
      "Words:\n",
      "['Dr.', 'John', 'Smith', 'Ph.D.', 'earned', '$1,250.50', 'on', 'Jan.', '15', '2024', 'for', 'his', 'work', 'at', 'A.I.', 'Corp.', 'You', 'can', 'reach', 'him', 'at', 'j.smith@ai-corp.co.uk', 'or', 'visit', 'https://www.ai-corp.co.uk/team/dr-smith', 'for', 'more', 'info.', 'The', 'U.S.A.-based', 'company', 'reported', 'a', '23.5%', 'increase', 'in', 'Q3', 'revenue', 'totaling', '€2.5M.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text with challenging elements\n",
    "text = \"\"\"Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "# Split the text into words\n",
    "words = split_text(text)\n",
    "print(\"\\nWords:\")\n",
    "print(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unix tools is not useful for this exercise\n",
    "# Unix tools like grep, sed, and awk are not suitable for this tokenization exercise because they lack linguistic intelligence\n",
    "# and work only with simple pattern matching. While we can extract words using regex patterns, these tools cannot reliably split\n",
    "# sentences because they treat all periods identically, incorrectly breaking abbreviations like \"Dr. Smith\" or \"Ph.D.\" into separate segments.\n",
    "#Unlike NLTK's Punkt algorithm or spaCy's neural models, which are trained on real text corpora to distinguish abbreviation periods from \n",
    "# sentence-ending periods, Unix tools have no contextual awareness or learning capability. We therefore exclude Unix tools because proper\n",
    "# sentence segmentation requires specialized linguistic knowledge that only dedicated NLP libraries provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences (NLTK):\n",
      "['Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I.', 'Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.', 'The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.']\n",
      "\n",
      "Words (NLTK):\n",
      "['Dr.', 'John', 'Smith', 'Ph.D.', 'earned', '$1,250.50', 'on', 'Jan.', '15', '2024', 'for', 'his', 'work', 'at', 'A.I.', 'Corp.', 'You', 'can', 'reach', 'him', 'at', 'j.smith@ai-corp.co.uk', 'or', 'visit', 'https://www.ai-corp.co.uk/team/dr-smith', 'for', 'more', 'info.', 'The', 'U.S.A.-based', 'company', 'reported', 'a', '23.5%', 'increase', 'in', 'Q3', 'revenue', 'totaling', '€2.5M.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\carid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Now we try with NLTK \n",
    "import nltk\n",
    "# This is needed to download the punkt tokenizer models\n",
    "nltk.download(\"punkt_tab\")\n",
    "#  We import the necessary functions\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "def nltk_split_text(text):\n",
    "    # We use NLTK's built-in functions to split the text, in this case in sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # We create a custom tokenizer using our specific regex pattern\n",
    "    # This pattern keeps dots in abbreviations and commas inside numbers\n",
    "    word_tokenizer = RegexpTokenizer(r\"[^\\s,]+(?:,[^\\s,]+)*\")\n",
    "    # And in words\n",
    "    words = word_tokenizer.tokenize(text)\n",
    "    return sentences, words\n",
    "\n",
    "# Now we try with NLTK\n",
    "sentences, words = nltk_split_text(text)\n",
    "print(\"\\nSentences (NLTK):\")\n",
    "print(sentences)\n",
    "print(\"\\nWords (NLTK):\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences (SpaCy):\n",
      "['Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp.', 'You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.', 'The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.']\n",
      "\n",
      "Words (SpaCy):\n",
      "['Dr.', 'John', 'Smith', 'Ph.D.', 'earned', '$1,250.50', 'on', 'Jan.', '15', '2024', 'for', 'his', 'work', 'at', 'A.I.', 'Corp.', 'You', 'can', 'reach', 'him', 'at', 'j.smith@ai-corp.co.uk', 'or', 'visit', 'https://www.ai-corp.co.uk/team/dr-smith', 'for', 'more', 'info', 'The', 'U.S.A.-based', 'company', 'reported', 'a', '23.5%', 'increase', 'in', 'Q3', 'revenue', 'totaling', '€2.5M.']\n"
     ]
    }
   ],
   "source": [
    "# Now we try with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def spacy_split_text(text):\n",
    "    # We process the text with spaCy's neural pipeline\n",
    "    doc = nlp(text)\n",
    "    # We extract sentences using dependency-based segmentation\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    # We filter out commas and standalone periods that are sentence-final punctuation\n",
    "    # We keep periods that are part of abbreviations (like Dr. or Ph.D.)\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        # We skip commas completely\n",
    "        if token.text == ',':\n",
    "            continue\n",
    "        # We skip periods that are standalone punctuation (sentence endings)\n",
    "        # SpaCy marks them with is_punct=True and they appear after spaces\n",
    "        elif token.text == '.' and token.is_punct:\n",
    "            continue\n",
    "        # Otherwise we keep the token\n",
    "        else:\n",
    "            words.append(token.text)\n",
    "\n",
    "    # We have also to take into account that $ needs to be attached to the number\n",
    "    # So we merge $ with the next token if applicable\n",
    "    # We create a new list to hold the filtered words\n",
    "    filtered_words = []\n",
    "    # We use a variable to skip the next token if we have merged it\n",
    "    skip_next = False\n",
    "    for i, token in enumerate(words):\n",
    "        # If we have to skip the next token, we do it\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        # If the token is $ or € , we merge it with the next token\n",
    "        if (token == \"$\" or token == \"€\") and i + 1 < len(words):\n",
    "            filtered_words.append(token + words[i + 1])\n",
    "            skip_next = True\n",
    "        \n",
    "        # If the next token is %, we merge it with the current number\n",
    "        elif i + 1 < len(words) and words[i + 1] == \"%\":\n",
    "            filtered_words.append(token + \"%\")\n",
    "            skip_next = True\n",
    "            \n",
    "        # Otherwise, we just add the token\n",
    "        else:\n",
    "            filtered_words.append(token)\n",
    "    words = filtered_words\n",
    "\n",
    "    \n",
    "    return sentences, words\n",
    "\n",
    "# Now we try with SpaCy\n",
    "sentences, words = spacy_split_text(text)\n",
    "print(\"\\nSentences (SpaCy):\")\n",
    "print(sentences)\n",
    "print(\"\\nWords (SpaCy):\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting words in large corpuses and find the most common words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a file `shakes.txt` in the book folder. Find the words that are more common in Shakespeare's book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 Most common words in 'shakes.txt' using NLTK:\n",
      "[('the', 27729), ('and', 26746), ('i', 19856), ('to', 18843), ('of', 18163), ('a', 14438), ('my', 12457), ('you', 12175), ('that', 10840), ('in', 10830)]\n"
     ]
    }
   ],
   "source": [
    "# Now we need to find the most common words in the text \"shakes.txt\", first with NLTK\n",
    "from collections import Counter\n",
    "\n",
    "# We read the text file\n",
    "with open(\"TXT_FILES/shakes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    shakes_text = f.read()\n",
    "\n",
    "# Now we create a function to get the most common words\n",
    "def most_common_words_nltk(text, n=10):\n",
    "    # We use the nltk_split_text function to get the words\n",
    "    _, words = nltk_split_text(text)\n",
    "    # We convert all words to lowercase for uniformity\n",
    "    words = [word.lower() for word in words]\n",
    "    # We use Counter to count the occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "    # We return the n most common words\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "# We get the 10 most common words in the shakes_text\n",
    "common_words_nltk = most_common_words_nltk(shakes_text, n=10)\n",
    "print(\"\\n10 Most common words in 'shakes.txt' using NLTK:\")\n",
    "print(common_words_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 5465395 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m word_counts.most_common(n)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# We get the 10 most common words in the shakes_text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m common_words_spacy = \u001b[43mmost_common_words_spacy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshakes_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m10 Most common words in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mshakes.txt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m using SpaCy:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(common_words_spacy)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mmost_common_words_spacy\u001b[39m\u001b[34m(text, n)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmost_common_words_spacy\u001b[39m(text, n=\u001b[32m10\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# We use the spacy_split_text function to get the words\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     _, words = \u001b[43mspacy_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# We convert all words to lowercase for uniformity\u001b[39;00m\n\u001b[32m      6\u001b[39m     words = [word.lower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mspacy_split_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mspacy_split_text\u001b[39m(text):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# We process the text with spaCy's neural pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# We extract sentences using dependency-based segmentation\u001b[39;00m\n\u001b[32m      8\u001b[39m     sentences = [sent.text \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc.sents]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carid\\Desktop\\UIE\\3º\\SEGUNDO CUATRIMESTRE\\Procesamiento del Lenguaje\\procesamiento\\Lib\\site-packages\\spacy\\language.py:1041\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1021\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1022\u001b[39m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1025\u001b[39m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1026\u001b[39m ) -> Doc:\n\u001b[32m   1027\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[33;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[33;03m    is preserved.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1039\u001b[39m \u001b[33;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m     doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1043\u001b[39m         component_cfg = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carid\\Desktop\\UIE\\3º\\SEGUNDO CUATRIMESTRE\\Procesamiento del Lenguaje\\procesamiento\\Lib\\site-packages\\spacy\\language.py:1132\u001b[39m, in \u001b[36mLanguage._ensure_doc\u001b[39m\u001b[34m(self, doc_like)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m.vocab).from_bytes(doc_like)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carid\\Desktop\\UIE\\3º\\SEGUNDO CUATRIMESTRE\\Procesamiento del Lenguaje\\procesamiento\\Lib\\site-packages\\spacy\\language.py:1121\u001b[39m, in \u001b[36mLanguage.make_doc\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[32m   1116\u001b[39m \n\u001b[32m   1117\u001b[39m \u001b[33;03mtext (str): The text to process.\u001b[39;00m\n\u001b[32m   1118\u001b[39m \u001b[33;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[38;5;28mself\u001b[39m.max_length:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1122\u001b[39m         Errors.E088.format(length=\u001b[38;5;28mlen\u001b[39m(text), max_length=\u001b[38;5;28mself\u001b[39m.max_length)\n\u001b[32m   1123\u001b[39m     )\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer(text)\n",
      "\u001b[31mValueError\u001b[39m: [E088] Text of length 5465395 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# Now we do it with SpaCy\n",
    "def most_common_words_spacy(text, n=10):\n",
    "    # We use the spacy_split_text function to get the words\n",
    "    _, words = spacy_split_text(text)\n",
    "    # We convert all words to lowercase for uniformity\n",
    "    words = [word.lower() for word in words]\n",
    "    # We use Counter to count the occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "    # We return the n most common words\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "# We get the 10 most common words in the shakes_text\n",
    "common_words_spacy = most_common_words_spacy(shakes_text, n=10)\n",
    "print(\"\\n10 Most common words in 'shakes.txt' using SpaCy:\")\n",
    "print(common_words_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we do it again with an increased max_length it will work better, beacuse Spacy for default has a max_length of 1 million characters\n",
    "\n",
    "# We increase spaCy's max_length to handle large texts\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 6000000  # We set it to 6 million characters (more than our text)\n",
    "\n",
    "def most_common_words_spacy(text, n=10):\n",
    "    # We use the spacy_split_text function to get the words\n",
    "    _, words = spacy_split_text(text)\n",
    "    # We convert all words to lowercase for uniformity\n",
    "    words = [word.lower() for word in words]\n",
    "    # We use Counter to count the occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "    # We return the n most common words\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "\n",
    "# We get the 10 most common words in the shakes_text\n",
    "common_words_spacy = most_common_words_spacy(shakes_text, n=10)\n",
    "print(\"\\n10 Most common words in 'shakes.txt' using SpaCy:\")\n",
    "print(common_words_spacy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
