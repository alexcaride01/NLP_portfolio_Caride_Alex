{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Words, Tokens, and Regular Expressions\n",
    "## Exercises Notebook - Session 3\n",
    "\n",
    "This notebook contains exercises covering:\n",
    "- Tokenization concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Tokenization Concepts\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Types vs Instances\n",
    "\n",
    "The slides distinguish between types and instances.\n",
    "For the given text, calculate:\n",
    "1. Number of instances (total tokens)\n",
    "2. Number of types (vocabulary size |V|)\n",
    "3. Type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances (total tokens): 10\n",
      "Number of types (vocabulary size |V|): 7\n",
      "Type-token ratio: 0.70\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "text = \"the cat sat on the mat the cat was fat\"\n",
    "\n",
    "# 1. Number of instances (total tokens)\n",
    "# We can split the text into tokens using whitespace\n",
    "tokens = text.split()\n",
    "# We count the total number of tokens\n",
    "num_instances = len(tokens)\n",
    "print(f\"Number of instances (total tokens): {num_instances}\")\n",
    "\n",
    "# 2. Number of types (vocabulary size |V|)\n",
    "# We can use a set to find unique tokens (types)\n",
    "types = set(tokens)\n",
    "# We count the number of unique types\n",
    "vocabulary_size = len(types)\n",
    "print(f\"Number of types (vocabulary size |V|): {vocabulary_size}\")\n",
    "\n",
    "# 3. Type-token ratio\n",
    "# We calculate the type-token ratio as vocabulary size divided by number of instances\n",
    "type_token_ratio = vocabulary_size / num_instances\n",
    "print(f\"Type-token ratio: {type_token_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Heaps' Law Demonstration\n",
    "\n",
    "The slides mention Heaps' Law: vocabulary size grows with âˆšN.\n",
    "\n",
    "Generate text of increasing length and observe vocabulary growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 100 tokens, Vocabulary size |V|: 28\n",
      "Text length: 200 tokens, Vocabulary size |V|: 29\n",
      "Text length: 300 tokens, Vocabulary size |V|: 29\n",
      "Text length: 400 tokens, Vocabulary size |V|: 29\n",
      "Text length: 500 tokens, Vocabulary size |V|: 29\n",
      "Text length: 600 tokens, Vocabulary size |V|: 29\n",
      "Text length: 700 tokens, Vocabulary size |V|: 29\n",
      "Text length: 800 tokens, Vocabulary size |V|: 29\n",
      "Text length: 900 tokens, Vocabulary size |V|: 29\n",
      "Text length: 1000 tokens, Vocabulary size |V|: 29\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import random\n",
    "\n",
    "# Use a simple word list to simulate text\n",
    "word_list = ['the', 'a', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "             'cat', 'dog', 'bird', 'fish', 'tree', 'house', 'car',\n",
    "             'run', 'walk', 'jump', 'eat', 'sleep', 'read', 'write',\n",
    "             'big', 'small', 'fast', 'slow', 'red', 'blue', 'green']\n",
    "\n",
    "# Generate text of increasing length and observe vocabulary growth.\n",
    "for length in range(100, 1100, 100):\n",
    "    # We generate random text of the specified length using random choices from the word list\n",
    "    generated_text = ' '.join(random.choices(word_list, k=length))\n",
    "    # We calculate the vocabulary size for the generated text\n",
    "    tokens = generated_text.split()\n",
    "    # We use a set to find unique tokens (types)\n",
    "    types = set(tokens)\n",
    "    # We count the number of unique types\n",
    "    vocabulary_size = len(types)\n",
    "    print(f\"Text length: {length} tokens, Vocabulary size |V|: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: BPE Simulation\n",
    "\n",
    "The slides explain Byte Pair Encoding (BPE).\n",
    "Implement a simple BPE token learner that:\n",
    "1. Starts with character vocabulary\n",
    "2. Finds most frequent adjacent pair\n",
    "3. Merges them into a new token\n",
    "4. Repeats k times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokens after merges: ['lowlow', 'e', 'r', 'n', 'e', 'w', 'est', 'w', 'i', 'd', 'est']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Starts with character vocabulary\n",
    "# 2. Finds most frequent adjacent pair\n",
    "# 3. Merges them into a new token\n",
    "# 4. Repeats k times\n",
    "corpus = \"low lower newest widest\"\n",
    "\n",
    "def simple_bpe(corpus, num_merges):\n",
    "    # We start with character vocabulary\n",
    "    tokens = list(corpus.replace(\" \", \"\"))\n",
    "    for _ in range(num_merges):\n",
    "        #  We need to find all adjacent pairs\n",
    "        pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "        #  Then we count frequency of each pair\n",
    "        pair_freq = {}\n",
    "        for pair in pairs:\n",
    "            # Here  we use .get to initialize count to 0 if pair not in dict\n",
    "            pair_freq[pair] = pair_freq.get(pair, 0) + 1\n",
    "        # We find the most frequent pair\n",
    "        if not pair_freq:\n",
    "            break\n",
    "        most_frequent_pair = max(pair_freq, key=pair_freq.get)\n",
    "        # We merge the most frequent pair\n",
    "        new_token = ''.join(most_frequent_pair)\n",
    "        new_tokens = []\n",
    "        skip = False\n",
    "        for i in range(len(tokens)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == most_frequent_pair:\n",
    "                new_tokens.append(new_token)\n",
    "                skip = True\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "        tokens = new_tokens\n",
    "    return tokens\n",
    "# Little explanation so I can understand the function:\n",
    "# This function simulates how we learn to recognize words by grouping letters together. Initially, the computer sees the text as separate\n",
    "# characters, like \"l\", \"o\", \"w\", \"e\", \"r\". It then scans the text to find which pair of letters appears together most frequently, \n",
    "# for example, \"o\" and \"w\". Once identified, it merges them into a single unit, \"ow\". In the next step, it might notice that \"l\" often \n",
    "# comes before \"ow\", so it combines them into a larger block, \"low\". The code repeats this process, gradually building a vocabulary of \n",
    "# common word parts based on the patterns it finds in the text.\n",
    "# We try our BPE\n",
    "bpe_tokens = simple_bpe(corpus, num_merges=5)\n",
    "print(\"BPE Tokens after merges:\", bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Advanced Regex\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Lookahead Assertions\n",
    "\n",
    "The slides introduce lookahead: (?=pattern) and (?!pattern)\n",
    "\n",
    "Write patterns to:\n",
    "1. Find words followed by a comma (without capturing comma)\n",
    "2. Find first word of line only if it doesn't start with 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words followed by a comma: ['quick']\n",
      "First words of lines not starting with 'T': []\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "text = \"The quick, brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# 1. Find words followed by a comma (without capturing comma)\n",
    "import re\n",
    "words_with_comma = re.findall(r\"\\b\\w+(?=,)\", text)\n",
    "print(\"Words followed by a comma:\", words_with_comma)\n",
    "\n",
    "# 2. Find first word of line only if it doesn't start with 'T'\n",
    "lines = text.split('. ')\n",
    "first_words_not_T = [re.match(r\"^(?!T)(\\w+)\", line).group(1) for line in lines if re.match(r\"^(?!T)(\\w+)\", line)]\n",
    "print(\"First words of lines not starting with 'T':\", first_words_not_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Non-capturing Groups\n",
    "\n",
    "The slides explain (?:...) for grouping without capturing.\n",
    "\n",
    "Write a pattern that matches \"some cats\" or \"a few cats\" \n",
    "but only captures \"cats\" (not \"some\" or \"a few\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 'cats' in: 'some cats like fish' : 'cats'\n",
      "Matched 'cats' in: 'a few cats play outside' : 'cats'\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "texts = [\n",
    "    \"some cats like fish\",\n",
    "    \"a few cats play outside\", \n",
    "    \"some dogs bark\"\n",
    "]\n",
    "\n",
    "# Write a pattern that matches \"some cats\" or \"a few cats\" but only captures \"cats\" (not \"some\" or \"a few\").\n",
    "pattern = r\"(?:some|a few) (cats)\"\n",
    "for text in texts:\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        print(f\"Matched 'cats' in: '{text}' : '{match.group(1)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: GPT-2 Pre-tokenization\n",
    "\n",
    "The slides show the GPT-2 pre-tokenization regex.\n",
    "Test the pattern and understand what each part does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 style tokens: ['I', \"'m\", 'learning', 'NLP', '!', 'It', \"'s\", 'fascinating', '.', 'I', \"'ve\", 'got', '100', 'examples', '.']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "gpt2_pattern = r\"'s|'t|'re|'ve|'m|'ll|'d|\\w+|\\d+|[^\\s\\w]+\"\n",
    "test = \"I'm learning NLP! It's fascinating. I've got 100 examples.\"\n",
    "# We test the pattern\n",
    "tokens = re.findall(gpt2_pattern, test)\n",
    "print(\"GPT-2 style tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Morphology\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Identifying Morphemes\n",
    "\n",
    "The slides define morphemes as minimal meaning-bearing units.\n",
    "\n",
    "Write code to identify potential morphemes by finding:\n",
    "1. Common suffixes (-ed, -ing, -s, -ly, -ful)\n",
    "2. Common prefixes (un-, re-, pre-, dis-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "words = [\"working\", \"unhappy\", \"carefully\", \"reworked\", \"glasses\", \"preprocessing\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
